const { Innertube } = require('youtubei.js');
const fs = require('fs').promises;
const path = require('path');
const { execSync } = require('child_process');
const zlib = require('zlib');
const { promisify } = require('util');

const gzip = promisify(zlib.gzip);
const gunzip = promisify(zlib.gunzip);

const CONFIG = {
  BATCH_SIZE: 10,
  MAX_RETRIES: 3,
  RETRY_DELAY: 2000,
  COMMIT_BATCH_SIZE: 100,
  MAX_RUNTIME_SECONDS: 55,  // Leave 5 seconds buffer for cron every minute
  RATE_LIMIT_DELAY: 1000,
  SHARD_SIZE: 1000,  // Videos per shard file
  INDEX_UPDATE_FREQUENCY: 100  // Update index every N videos
};

class YouTubeScraperV2 {
  constructor() {
    this.youtube = null;
    this.startTime = Date.now();
    this.processedCount = 0;
    this.failedUrls = [];
    this.currentShard = null;
    this.shardBuffer = [];
    this.indexBuffer = [];
  }

  async initialize() {
    this.youtube = await Innertube.create({
      retrieve_player: true,
      enable_safety_mode: false
    });
    await this.loadCurrentShard();
  }

  formatTime(ms) {
    const totalSeconds = Math.floor(ms / 1000);
    const hours = Math.floor(totalSeconds / 3600);
    const minutes = Math.floor((totalSeconds % 3600) / 60);
    const seconds = totalSeconds % 60;
    const milliseconds = ms % 1000;
    return `${String(hours).padStart(2, '0')}:${String(minutes).padStart(2, '0')}:${String(seconds).padStart(2, '0')},${String(milliseconds).padStart(3, '0')}`;
  }

  generateSRT(segments) {
    return segments.map((seg, i) => {
      const start = this.formatTime(Number(seg.s) || 0);
      const end = this.formatTime(Number(seg.e) || 0);
      return `${i + 1}\n${start} --> ${end}\n${seg.t}\n`;
    }).join('\n');
  }

  extractVideoId(url) {
    const patterns = [
      /(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([^&\n?#]+)/,
      /^([a-zA-Z0-9_-]{11})$/
    ];

    for (const pattern of patterns) {
      const match = url.match(pattern);
      if (match) return match[1];
    }
    return null;
  }

  getShardPath(videoId) {
    // Use first 2 chars for directory sharding (1296 possible dirs)
    const prefix = videoId.substring(0, 2).toLowerCase();
    const shardNum = Math.floor(this.hashCode(videoId) % 1000);
    return `data/shards/${prefix}/${shardNum}.jsonl.gz`;
  }

  hashCode(str) {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash;
    }
    return Math.abs(hash);
  }

  async loadCurrentShard() {
    try {
      const shardInfoPath = 'data/current-shard.json';
      const shardInfo = JSON.parse(await fs.readFile(shardInfoPath, 'utf-8'));
      this.currentShard = shardInfo;
    } catch {
      this.currentShard = {
        path: null,
        count: 0,
        videos: new Set()
      };
    }
  }

  async checkIfProcessed(videoId) {
    try {
      // Check bloom filter or index first (for speed)
      const indexPath = 'data/index/master.json';
      const index = JSON.parse(await fs.readFile(indexPath, 'utf-8'));
      return index.processed.includes(videoId);
    } catch {
      return false;
    }
  }

  async scrapeVideo(videoUrl, retries = 0) {
    const videoId = this.extractVideoId(videoUrl);

    if (!videoId) {
      console.error(`Invalid YouTube URL: ${videoUrl}`);
      return null;
    }

    if (await this.checkIfProcessed(videoId)) {
      console.log(`Skipping ${videoId} - already processed`);
      return { id: videoId, skipped: true };
    }

    try {
      console.log(`Processing video: ${videoId} (attempt ${retries + 1})`);

      const info = await this.youtube.getInfo(videoId);

      const videoData = {
        id: videoId,
        title: info.basic_info.title,
        author: info.basic_info.author,
        duration: info.basic_info.duration,
        view_count: info.basic_info.view_count,
        upload_date: info.primary_info?.published?.text || null,
        url: videoUrl,
        scraped_at: new Date().toISOString()
      };

      const captionTracks = info.captions?.caption_tracks || [];
      console.log(`Found ${captionTracks.length} caption tracks`);

      const captions = {};

      // Helper to map transcript segments to compact format
      const mapSegments = (segments) => segments.map(s => ({
        s: s.start_ms || 0,
        e: s.end_ms || 0,
        t: s.snippet ? s.snippet.text : (s.text || '')
      }));

      // Try official transcript API once and reuse by selecting languages
      let transcriptInfo = null;
      try {
        transcriptInfo = await info.getTranscript();
      } catch (err) {
        console.warn(`getTranscript API unavailable for ${videoId}: ${err.message}`);
      }

      for (const track of captionTracks) {
        const languageCode = track.language_code;
        const isAutoGenerated = track.kind === 'asr';

        // Prefer human-created tracks; we'll still process ASR if humans are absent
        try {
          let segments = null;

          // Primary: YouTube transcript API
          if (transcriptInfo) {
            try {
              const langs = transcriptInfo.languages || [];
              const selected = langs.includes(languageCode)
                ? await transcriptInfo.selectLanguage(languageCode)
                : transcriptInfo;
              const transcript = selected.transcript;
              const segs = transcript?.content?.body?.initial_segments || [];
              if (segs.length) {
                segments = mapSegments(segs);
              }
            } catch (e) {
              console.warn(`Transcript API language switch failed (${languageCode}): ${e.message}`);
            }
          }

          // No fallback: if transcript API didn't yield segments, skip this language

          if (segments && segments.length) {
            captions[languageCode] = {
              auto: isAutoGenerated,
              segments
            };
          }
        } catch (error) {
          console.error(`Failed to download ${languageCode} captions:`, error.message);
        }

        await this.delay(100);
      }

      const record = {
        ...videoData,
        captions
      };

      // Do not write API artifacts here; we'll generate after scrape

      // Add to shard buffer
      this.shardBuffer.push(record);
      this.indexBuffer.push(videoId);

      this.processedCount++;
      return videoData;

    } catch (error) {
      console.error(`Failed to process ${videoId}:`, error.message);

      if (retries < CONFIG.MAX_RETRIES) {
        await this.delay(CONFIG.RETRY_DELAY * (retries + 1));
        return this.scrapeVideo(videoUrl, retries + 1);
      }

      this.failedUrls.push({ url: videoUrl, error: error.message });
      return null;
    }
  }

  async flushShardBuffer() {
    if (this.shardBuffer.length === 0) return;

    // Group by shard
    const shards = {};
    for (const record of this.shardBuffer) {
      const shardPath = this.getShardPath(record.id);
      if (!shards[shardPath]) {
        shards[shardPath] = [];
      }
      shards[shardPath].push(record);
    }

    // Write to each shard
    for (const [shardPath, records] of Object.entries(shards)) {
      await this.appendToShard(shardPath, records);
    }

    // Update index
    await this.updateIndex(this.indexBuffer);

    // Clear buffers
    this.shardBuffer = [];
    this.indexBuffer = [];
  }

  async appendToShard(shardPath, records) {
    const dir = path.dirname(shardPath);
    await fs.mkdir(dir, { recursive: true });

    let existingData = '';
    try {
      const compressed = await fs.readFile(shardPath);
      const decompressed = await gunzip(compressed);
      existingData = decompressed.toString();
    } catch {
      // File doesn't exist yet
    }

    const newLines = records.map(r => JSON.stringify(r)).join('\n');
    const allData = existingData + (existingData ? '\n' : '') + newLines;

    const compressed = await gzip(allData);
    await fs.writeFile(shardPath, compressed);

    console.log(`Added ${records.length} records to ${shardPath}`);
  }

  async updateIndex(videoIds) {
    const indexDir = 'data/index';
    await fs.mkdir(indexDir, { recursive: true });

    const masterIndexPath = path.join(indexDir, 'master.json');
    let masterIndex;

    try {
      masterIndex = JSON.parse(await fs.readFile(masterIndexPath, 'utf-8'));
    } catch {
      masterIndex = {
        total: 0,
        processed: [],
        shards: {},
        updated: new Date().toISOString()
      };
    }

    // Add new video IDs
    const newProcessed = new Set([...masterIndex.processed, ...videoIds]);
    masterIndex.processed = Array.from(newProcessed);
    masterIndex.total = masterIndex.processed.length;
    masterIndex.updated = new Date().toISOString();

    // Update shard index
    for (const videoId of videoIds) {
      const shardPath = this.getShardPath(videoId);
      if (!masterIndex.shards[shardPath]) {
        masterIndex.shards[shardPath] = [];
      }
      masterIndex.shards[shardPath].push(videoId);
    }

    await fs.writeFile(masterIndexPath, JSON.stringify(masterIndex, null, 2));

    // Create date-based index for quick lookups
    const dateIndex = `${new Date().toISOString().split('T')[0]}.json`;
    await fs.writeFile(
      path.join(indexDir, dateIndex),
      JSON.stringify(videoIds, null, 2)
    );
  }

  delay(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  isTimeUp() {
    const elapsed = Date.now() - this.startTime;
    return elapsed > CONFIG.MAX_RUNTIME_SECONDS * 1000;
  }

  async processBatch(urls) {
    const results = [];

    for (const url of urls) {
      if (this.isTimeUp()) {
        console.log('Time limit reached, stopping batch processing');
        break;
      }

      const result = await this.scrapeVideo(url);
      if (result && !result.skipped) {
        results.push(result);
      }

      // Flush buffers periodically
      if (this.shardBuffer.length >= CONFIG.SHARD_SIZE) {
        await this.flushShardBuffer();
        await this.commitChanges();
      }

      await this.delay(CONFIG.RATE_LIMIT_DELAY);
    }

    return results;
  }

  async commitChanges() {
    if (!process.env.GITHUB_ACTIONS) return;

    try {
      execSync('git config user.name "GitHub Actions Bot"');
      execSync('git config user.email "actions@github.com"');
      execSync('git add data/');

      const message = `Update captions (${this.processedCount} videos) [skip ci]`;
      execSync(`git commit -m "${message}"`);
      execSync('git push');

      console.log(`Committed ${this.processedCount} videos`);
    } catch (error) {
      console.error('Failed to commit changes:', error.message);
    }
  }
}

// Utility to read from shards
class ShardReader {
  async getVideo(videoId) {
    const scraper = new YouTubeScraperV2();
    const shardPath = scraper.getShardPath(videoId);

    try {
      const compressed = await fs.readFile(shardPath);
      const decompressed = await gunzip(compressed);
      const lines = decompressed.toString().split('\n');

      for (const line of lines) {
        if (!line) continue;
        const record = JSON.parse(line);
        if (record.id === videoId) {
          return record;
        }
      }
    } catch (error) {
      console.error(`Failed to read shard: ${error.message}`);
    }

    return null;
  }

  async exportVideoData(videoId, outputDir) {
    const video = await this.getVideo(videoId);
    if (!video) {
      console.log(`Video ${videoId} not found`);
      return;
    }

    const videoDir = path.join(outputDir, videoId);
    await fs.mkdir(videoDir, { recursive: true });

    // Write metadata
    const metadata = { ...video };
    delete metadata.captions;
    await fs.writeFile(
      path.join(videoDir, 'metadata.json'),
      JSON.stringify(metadata, null, 2)
    );

    // Write captions
    for (const [lang, data] of Object.entries(video.captions || {})) {
      const prefix = data.auto ? 'auto_' : '';

      // Generate SRT
      const srt = data.segments.map((seg, i) => {
        const start = this.formatTime(seg.s);
        const end = this.formatTime(seg.e);
        return `${i + 1}\n${start} --> ${end}\n${seg.t}\n`;
      }).join('\n');

      await fs.writeFile(
        path.join(videoDir, `${prefix}${lang}.srt`),
        srt
      );

      // Generate plain text
      const txt = data.segments.map(seg => seg.t).join(' ');
      await fs.writeFile(
        path.join(videoDir, `${prefix}${lang}.txt`),
        txt
      );
    }

    console.log(`Exported ${videoId} to ${videoDir}`);
  }

  formatTime(ms) {
    const totalSeconds = Math.floor(ms / 1000);
    const hours = Math.floor(totalSeconds / 3600);
    const minutes = Math.floor((totalSeconds % 3600) / 60);
    const seconds = totalSeconds % 60;
    const milliseconds = ms % 1000;

    return `${String(hours).padStart(2, '0')}:${String(minutes).padStart(2, '0')}:${String(seconds).padStart(2, '0')},${String(milliseconds).padStart(3, '0')}`;
  }
}

async function getUnprocessedUrls() {
  const urlsFile = 'urls.txt';
  try {
    const content = await fs.readFile(urlsFile, 'utf-8');
    const urls = content.split('\n').filter(url => url.trim());
    return urls;
  } catch {
    return [];
  }
}

async function main() {
  try {
    const scraper = new YouTubeScraperV2();
    await scraper.initialize();

    // Check for more URLs every 5 seconds until timeout
    let totalProcessed = 0;
    while (!scraper.isTimeUp()) {
      const urls = await getUnprocessedUrls();

      if (urls.length === 0) {
        console.log('No URLs to process, waiting 5 seconds for more...');
        await new Promise(resolve => setTimeout(resolve, 5000));
        continue;
      }

      console.log(`Found ${urls.length} URLs to process`);

      for (let i = 0; i < urls.length; i += CONFIG.BATCH_SIZE) {
        if (scraper.isTimeUp()) {
          console.log('Time limit reached, stopping');
          break;
        }

        const batch = urls.slice(i, i + CONFIG.BATCH_SIZE);
        await scraper.processBatch(batch);
        totalProcessed += batch.length;

        console.log(`Progress: ${Math.min(i + CONFIG.BATCH_SIZE, urls.length)}/${urls.length} URLs processed this round`);
      }

      // Clear processed URLs
      try {
        await fs.writeFile('urls.txt', '');
      } catch (e) {
        console.warn('Could not clear urls.txt:', e.message);
      }

      console.log(`Completed processing round. Total processed: ${totalProcessed}`);
    }

    // Final flush
    await scraper.flushShardBuffer();
    await scraper.commitChanges();

    // Always regenerate static API after scrape
    try {
      const { StaticAPIGenerator } = require('./scripts/generate-static-api');
      const generator = new StaticAPIGenerator();
      await generator.generate();
    } catch (e) {
      console.error('Failed to regenerate static API:', e.message);
    }

    console.log(`
Summary:
- Total processed: ${totalProcessed} videos
- Failed: ${scraper.failedUrls.length} URLs
    `);

  } catch (error) {
    console.error('Fatal error:', error);
    process.exit(1);
  }
}

if (require.main === module) {
  main();
}

module.exports = { YouTubeScraperV2, ShardReader };